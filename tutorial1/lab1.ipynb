{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session 1: Environment setup and feedforward networks in PyTorch\n",
    "\n",
    "#### Lab setup instructions\n",
    "\n",
    "We recommend that you work on the Ubuntu workstations in the lab. We cannot guarantee compatibility with Windows machines and cannot promise support if you choose to work on a Windows machine.\n",
    "\n",
    "Once logged in, run the following commands in the terminal to set up a Python environment with all the packages you will need.\n",
    "\n",
    "    export PYTHONUSERBASE=/vol/bitbucket/nuric/pypi\n",
    "    export PATH=/vol/bitbucket/nuric/pypi/bin:$PATH\n",
    "\n",
    "Add the above lines to your `.bashrc` to have these enviroment variables set automatically each time you open your bash terminal.\n",
    "\n",
    "Run `jupyter-notebook` in the directory in which you've cloned the tutorial repo to launch Jupyter notebook in your default browser.\n",
    "\n",
    "This may take a while to run the first time while the packages are cached on your local machine.\n",
    "\n",
    "DO NOT attempt to create a virtualenv in your home folder as you will likely exceed your file quota.\n",
    "\n",
    "#### Google Colaboratory\n",
    "\n",
    "Alternatively, you can use Google Colaboratory, which provides free GPU time.\n",
    "\n",
    "You will need a Google account to do so. Simply log in to your account and go to the following page: https://colab.research.google.com\n",
    "\n",
    "You will need to install a few packages only if you choose to use Colab. Run the following command in a Colab notebook cell to install pytorch:\n",
    "\n",
    "    !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Your task is to replicate what was shown in the demonstration: use PyTorch to implement a one-hidden layer neural network to classify MNIST digits.\n",
    "\n",
    "HINT: you may find the following imports useful:\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    from torchvision import datasets, transforms\n",
    "    \n",
    "Try to consult the PyTorch documentation if you get stuck. We will also be available in the lab to answer questions.\n",
    "\n",
    "Experiment with different hyperparameter settings and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7d6f132a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a seed to ensure results are reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHiddenLayerMNISTClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden_layers=32, IN_FEATURES=784, OUT_FEATURES=10):\n",
    "        super().__init__()\n",
    "        self.IN_FEATURES = IN_FEATURES\n",
    "        self.OUT_FEATURES = OUT_FEATURES\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_features=self.IN_FEATURES, out_features=n_hidden_layers, bias=True\n",
    "        )\n",
    "        self.linear2 = nn.Linear(\n",
    "            in_features=n_hidden_layers, out_features=self.OUT_FEATURES, bias=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data = self.linear1(data.view(-1, self.IN_FEATURES))\n",
    "        data = F.relu(data)\n",
    "        data = self.linear2(data)\n",
    "        return F.log_softmax(data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch, log_interval=100):\n",
    "    \"\"\"\n",
    "    A utility function that performs a basic training loop.\n",
    "\n",
    "    For each batch in the training set, fetched using `train_loader`:\n",
    "        - Zeroes the gradient used by `optimizer`\n",
    "        - Performs forward pass through `model` on the given batch\n",
    "        - Computes loss on batch\n",
    "        - Performs backward pass\n",
    "        - `optimizer` updates model parameters using computed gradient\n",
    "\n",
    "    Prints the training loss on the current batch every `log_interval` batches.\n",
    "    \"\"\"\n",
    "    for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "        # Set previously computed gradients to zero, i.e. do not accumulate\n",
    "        optimizer.zero_grad()\n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "        # Calculate negative log likelihood loss\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        # Propagate the errors, i.e. compute the new gradients\n",
    "        loss.backward()\n",
    "        # Update the parameters using the new gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {:02d} -- Batch: {:03d} -- Loss: {:.4f}\".format(\n",
    "                    epoch,\n",
    "                    batch_index,\n",
    "                    # Calling `loss.item()` returns the scalar loss as a Python\n",
    "                    # number.\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \"\"\"\n",
    "    A utility function to compute the loss and accuracy on a test set by\n",
    "    iterating through the test set using the provided `test_loader` and\n",
    "    accumulating the loss and accuracy on each batch.\n",
    "    \"\"\"\n",
    "    test_loss = 0.\n",
    "    correct_predictions = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Make the predictions\n",
    "            outputs = model(inputs)\n",
    "            # Compute the sum of losses\n",
    "            test_loss += F.nll_loss(outputs, labels, reduction=\"sum\").item()\n",
    "            # Get predictions with highest probabilities\n",
    "            predictions = outputs.argmax(dim=1, keepdim=True)\n",
    "            # Sum the correct predictions\n",
    "            correct_predictions += predictions.eq(labels.view_as(predictions)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct_predictions /= len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {:.4f}\\n\".format(\n",
    "            test_loss, correct_predictions\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7d2c24c6a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]#, transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    \"train_data/\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    \"test_data/\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "sample_data, sample_label = train_data[0]\n",
    "\n",
    "print(sample_label)\n",
    "print(sample_data.size())\n",
    "print(sample_data.squeeze().size())\n",
    "\n",
    "plt.imshow(sample_data.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64]) \n",
      "\n",
      "tensor([5, 6, 5, 8, 9, 3, 8, 0, 5, 4, 1, 5, 1, 3, 2, 2, 7, 6, 4, 0, 3, 8, 5, 6,\n",
      "        6, 2, 1, 1, 8, 3, 1, 0, 7, 1, 3, 3, 2, 7, 3, 2, 3, 3, 2, 0, 5, 3, 7, 5,\n",
      "        2, 3, 9, 7, 5, 2, 2, 8, 7, 3, 6, 1, 1, 7, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader is good for iterating over shuffled data\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1024, shuffle=False)\n",
    "\n",
    "sample_inputs, sample_targets = next(iter(train_loader))\n",
    "\n",
    "print(sample_inputs.shape, sample_targets.shape, '\\n')\n",
    "print(sample_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 00 -- Batch: 000 -- Loss: 2.3109\n",
      "Train Epoch: 00 -- Batch: 100 -- Loss: 0.8254\n",
      "Train Epoch: 00 -- Batch: 200 -- Loss: 0.3472\n",
      "Train Epoch: 00 -- Batch: 300 -- Loss: 0.4184\n",
      "Train Epoch: 00 -- Batch: 400 -- Loss: 0.3117\n",
      "Train Epoch: 00 -- Batch: 500 -- Loss: 0.3917\n",
      "Train Epoch: 00 -- Batch: 600 -- Loss: 0.2079\n",
      "Train Epoch: 00 -- Batch: 700 -- Loss: 0.2580\n",
      "Train Epoch: 00 -- Batch: 800 -- Loss: 0.2118\n",
      "Train Epoch: 00 -- Batch: 900 -- Loss: 0.2659\n",
      "\n",
      "Test set: Average loss: 0.2515, Accuracy: 0.9264\n",
      "\n",
      "Train Epoch: 01 -- Batch: 000 -- Loss: 0.1736\n",
      "Train Epoch: 01 -- Batch: 100 -- Loss: 0.3511\n",
      "Train Epoch: 01 -- Batch: 200 -- Loss: 0.1825\n",
      "Train Epoch: 01 -- Batch: 300 -- Loss: 0.3883\n",
      "Train Epoch: 01 -- Batch: 400 -- Loss: 0.2972\n",
      "Train Epoch: 01 -- Batch: 500 -- Loss: 0.2620\n",
      "Train Epoch: 01 -- Batch: 600 -- Loss: 0.2584\n",
      "Train Epoch: 01 -- Batch: 700 -- Loss: 0.2006\n",
      "Train Epoch: 01 -- Batch: 800 -- Loss: 0.2944\n",
      "Train Epoch: 01 -- Batch: 900 -- Loss: 0.4673\n",
      "\n",
      "Test set: Average loss: 0.1920, Accuracy: 0.9437\n",
      "\n",
      "Train Epoch: 02 -- Batch: 000 -- Loss: 0.1609\n",
      "Train Epoch: 02 -- Batch: 100 -- Loss: 0.3883\n",
      "Train Epoch: 02 -- Batch: 200 -- Loss: 0.1689\n",
      "Train Epoch: 02 -- Batch: 300 -- Loss: 0.1976\n",
      "Train Epoch: 02 -- Batch: 400 -- Loss: 0.1961\n",
      "Train Epoch: 02 -- Batch: 500 -- Loss: 0.3933\n",
      "Train Epoch: 02 -- Batch: 600 -- Loss: 0.1094\n",
      "Train Epoch: 02 -- Batch: 700 -- Loss: 0.1826\n",
      "Train Epoch: 02 -- Batch: 800 -- Loss: 0.0509\n",
      "Train Epoch: 02 -- Batch: 900 -- Loss: 0.2224\n",
      "\n",
      "Test set: Average loss: 0.1644, Accuracy: 0.9530\n",
      "\n",
      "Train Epoch: 03 -- Batch: 000 -- Loss: 0.1027\n",
      "Train Epoch: 03 -- Batch: 100 -- Loss: 0.1095\n",
      "Train Epoch: 03 -- Batch: 200 -- Loss: 0.2459\n",
      "Train Epoch: 03 -- Batch: 300 -- Loss: 0.0596\n",
      "Train Epoch: 03 -- Batch: 400 -- Loss: 0.0552\n",
      "Train Epoch: 03 -- Batch: 500 -- Loss: 0.1401\n",
      "Train Epoch: 03 -- Batch: 600 -- Loss: 0.2836\n",
      "Train Epoch: 03 -- Batch: 700 -- Loss: 0.3387\n",
      "Train Epoch: 03 -- Batch: 800 -- Loss: 0.1006\n",
      "Train Epoch: 03 -- Batch: 900 -- Loss: 0.2235\n",
      "\n",
      "Test set: Average loss: 0.1556, Accuracy: 0.9534\n",
      "\n",
      "Train Epoch: 04 -- Batch: 000 -- Loss: 0.0744\n",
      "Train Epoch: 04 -- Batch: 100 -- Loss: 0.1624\n",
      "Train Epoch: 04 -- Batch: 200 -- Loss: 0.1008\n",
      "Train Epoch: 04 -- Batch: 300 -- Loss: 0.0814\n",
      "Train Epoch: 04 -- Batch: 400 -- Loss: 0.3971\n",
      "Train Epoch: 04 -- Batch: 500 -- Loss: 0.1730\n",
      "Train Epoch: 04 -- Batch: 600 -- Loss: 0.1283\n",
      "Train Epoch: 04 -- Batch: 700 -- Loss: 0.0730\n",
      "Train Epoch: 04 -- Batch: 800 -- Loss: 0.1273\n",
      "Train Epoch: 04 -- Batch: 900 -- Loss: 0.0989\n",
      "\n",
      "Test set: Average loss: 0.1379, Accuracy: 0.9584\n",
      "\n",
      "Train Epoch: 05 -- Batch: 000 -- Loss: 0.1085\n",
      "Train Epoch: 05 -- Batch: 100 -- Loss: 0.1823\n",
      "Train Epoch: 05 -- Batch: 200 -- Loss: 0.1248\n",
      "Train Epoch: 05 -- Batch: 300 -- Loss: 0.2410\n",
      "Train Epoch: 05 -- Batch: 400 -- Loss: 0.0872\n",
      "Train Epoch: 05 -- Batch: 500 -- Loss: 0.0410\n",
      "Train Epoch: 05 -- Batch: 600 -- Loss: 0.0994\n",
      "Train Epoch: 05 -- Batch: 700 -- Loss: 0.0430\n",
      "Train Epoch: 05 -- Batch: 800 -- Loss: 0.0242\n",
      "Train Epoch: 05 -- Batch: 900 -- Loss: 0.0805\n",
      "\n",
      "Test set: Average loss: 0.1251, Accuracy: 0.9616\n",
      "\n",
      "Train Epoch: 06 -- Batch: 000 -- Loss: 0.0820\n",
      "Train Epoch: 06 -- Batch: 100 -- Loss: 0.1882\n",
      "Train Epoch: 06 -- Batch: 200 -- Loss: 0.1734\n",
      "Train Epoch: 06 -- Batch: 300 -- Loss: 0.1689\n",
      "Train Epoch: 06 -- Batch: 400 -- Loss: 0.0914\n",
      "Train Epoch: 06 -- Batch: 500 -- Loss: 0.1898\n",
      "Train Epoch: 06 -- Batch: 600 -- Loss: 0.0472\n",
      "Train Epoch: 06 -- Batch: 700 -- Loss: 0.0557\n",
      "Train Epoch: 06 -- Batch: 800 -- Loss: 0.1254\n",
      "Train Epoch: 06 -- Batch: 900 -- Loss: 0.1088\n",
      "\n",
      "Test set: Average loss: 0.1257, Accuracy: 0.9619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instance of a model\n",
    "model = OneHiddenLayerMNISTClassifier()\n",
    "\n",
    "# Instance of optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# Train-test loop\n",
    "for epoch in range(7):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "torch.save(model.state_dict(), \"mnist_custom_model.pt\")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"mnist_custom_model.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
